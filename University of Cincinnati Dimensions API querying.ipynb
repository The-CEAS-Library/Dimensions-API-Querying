{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Note to user:\n",
        " - The following is a summary of findings using a small subset of faculty/researchers from the University of Cincinnati\n",
        "\n",
        "First we discovered that Dimensions can store individuals under multiple Researcher IDs, making a little legwork necessary to verify they are all the same person\n",
        "\n",
        "Additionally, the researchers in question can be stored in the database with different first names. By this I mean some may have a preferred name, a middle initial with their first name, or possibly even only their first initial\n",
        "\n",
        "Another finding is that Dimensions is not immediate in logging publications to a given researcher ID, they have to verify it is the same person, and failing to do so leads to the existance of multiple IDs. This also means that some publications will not be found because we are searching for only UC affiliations and some of these temporary identities is missing the affiliation to UC. We can be confident that anything from before a year ago is logged with the appropriate affiliations. Following that, the next 6 months are less certain and anything recently published (around a month or two) will not be in the system at all."
      ],
      "metadata": {
        "id": "dN6QGSDIK_E9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Set up the dimensions API and required imports and log in"
      ],
      "metadata": {
        "id": "twfe0rJzVMWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 1:\n",
        "- use input() to ask for the key for you to paste in each time"
      ],
      "metadata": {
        "id": "8IgydSJfDNNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dimcli -U --quiet\n",
        "\n",
        "import dimcli\n",
        "from dimcli.utils import *\n",
        "import json\n",
        "import sys\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "print(\"==\\nLogging in..\")\n",
        "# https://digital-science.github.io/dimcli/getting-started.html#authentication\n",
        "ENDPOINT = \"https://app.dimensions.ai\"\n",
        "if 'google.colab' in sys.modules:\n",
        "  import getpass\n",
        "  KEY = input(\"Input your API key for this session: \")\n",
        "  dimcli.login(key=KEY, endpoint=ENDPOINT)\n",
        "else:\n",
        "  KEY = \"\"\n",
        "  dimcli.login(key=KEY, endpoint=ENDPOINT)\n",
        "dsl = dimcli.Dsl()"
      ],
      "metadata": {
        "id": "Cl_XUYygus_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 2:\n",
        "- save the key as a Secret directly to google Colab.\n",
        "\n",
        "The Notebook will ask for permission to access it and once granted will use the saved value\n",
        "\n",
        "To create a Secret, click the sideways key on the left side of the screen (its below {x} and above the folder icon)\n",
        "\n",
        "Once there give your secret a name such as Dimension_API_Key or one of your choosing\n",
        "\n",
        "Assign the value as your API key (dont add quotation marks)\n",
        "\n",
        "With that, your key is now saved to your google account to be used whenever."
      ],
      "metadata": {
        "id": "xGiC0WSrDV3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dimcli -U --quiet\n",
        "\n",
        "import dimcli\n",
        "from dimcli.utils import *\n",
        "import json\n",
        "import sys\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "print(\"==\\nLogging in..\")\n",
        "from google.colab import userdata\n",
        "dimcli.login(key=userdata.get('DimensionKey'), endpoint=\"https://app.dimensions.ai\")\n",
        "dsl = dimcli.Dsl()"
      ],
      "metadata": {
        "id": "PSlPagLACN9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3-4SIX13Clcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4mLk3fxqigQ"
      },
      "source": [
        "First, install the dimcli package to interact with the Dimensions API.\n",
        "- This command installs the `dimcli` package, which provides the necessary tools to interact with the Dimensions database. The `-U` flag ensures that you install or update to the latest version, and the `--quiet` flag suppresses unnecessary output during installation.\n",
        "\n",
        "Second, we will import the libraries and modules necessary for using the Dimensions API.\n",
        "- `dimcli`: The library used to interact with the Dimensions API.\n",
        "- `json`: A library for parsing JSON data returned by the API.\n",
        "- `sys`: Used for handling system-specific parameters, like checking if the notebook is running in Google Colab.\n",
        "- `pandas`: A powerful data manipulation library, useful for handling and analyzing datasets.\n",
        "- `numpy`: A library for numerical operations, which can be helpful for efficient data manipulation, calculations, or creating arrays.\n",
        "\n",
        "Third, to use the Dimensions API, you need to authenticate with your personal API key. In this step, you'll input your API key to log in.\n",
        "- The script checks if itâ€™s running in Google Colab using `sys.modules`.\n",
        "- If it is in Google Colab, it will prompt you to input your API key using `input()`. You can securely enter the key in Colab.\n",
        "- The `dimcli.login()` function authenticates with the API using the provided API key.\n",
        "- The `Dsl()` function creates an instance of the Dimensions API's Domain-Specific Language (DSL), which will be used to query the database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ld2Y5i2aVI6"
      },
      "source": [
        "## Step 2: Load Faculty Data and Prepare Lists of Faculty Members\n",
        "In this step, we will read an Excel file containing the faculty roster and organize the data into dictionaries and lists that will be used for further analysis.\n",
        "\n",
        "We start by reading the Excel file that contains the faculty data. This file includes data for various years across different sheets.\n",
        "- The `pd.read_excel()` function loads the Excel file `'Faculty Roster_Pillay Request_11.06.2024.xlsx'`.\n",
        "- The `['11.01.2018', '11.01.2019', ...]` argument specifies the list of sheet names to read from the Excel file, which correspond to different years.\n",
        "- The result is stored in `faculty_data`, a dictionary where each key is a year (e.g., `'11.01.2018'`), and the corresponding value is the data for that year in the form of a DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://raw.githubusercontent.com/The-CEAS-Library/Dimensions-API-Querying/master/Faculty%20Roster_Pillay%20Request_11.06.2024.xlsx' -O Faculty_Roster.xlsx\n",
        "faculty_data = pd.read_excel('Faculty_Roster.xlsx',['11.01.2018', '11.01.2019', '11.01.2020','11.01.2021','11.01.2022','11.01.2023','11.01.2024'])\n",
        "\n",
        "\n",
        "\n",
        "# Next, we initialize empty lists to store the faculty members' names in different formats.\n",
        "fullName, fName, lName, empNameAgg = [], [], [], []\n",
        "# fullName: A list for storing the full name of each faculty member (first and last).\n",
        "# fName: A list for storing the first names of faculty members.\n",
        "# lName: A list for storing the last names of faculty members.\n",
        "# empNameAgg: An aggregated list of all unique names across years.\n",
        "\n",
        "\n",
        "\n",
        "# We now extract and clean the employee names for each year and store them in a dictionary.\n",
        "# The names are cleaned by removing any periods, and the resulting list is sorted.\n",
        "\n",
        "# List of years for each date\n",
        "years = ['2018', '2019', '2020', '2021', '2022', '2023', '2024']\n",
        "\n",
        "# Gather and clean employee names by year\n",
        "#\n",
        "empNames = {f'{year}': sorted(name.replace('.', '') for name in faculty_data[f'11.01.{year}']['Employee']) for year in years }\n",
        "\n",
        "# empNames: A dictionary where each key is a year (e.g., `'2018'`), and the value is a sorted list of employee names after cleaning.\n",
        "# The names are cleaned by removing periods, (`name.replace('.', '')`), because the database does not contain them in the names.\n",
        "\n",
        "\n",
        "\n",
        "#Next, we aggregate all unique faculty names from each year into one sorted list.\n",
        "empNameAgg = sorted(set(name for names in empNames.values() for name in names))\n",
        "\n",
        "# This step flattens all the name lists across years and removes duplicates using set().\n",
        "# The result is a sorted list of unique faculty member names stored in empNameAgg.\n",
        "\n",
        "# We will now extract and organize the first names, last names, and full names from all the sheets in faculty_data.\n",
        "\n",
        "# Extract and organize name components across all sheets in faculty_data\n",
        "for sheet_data in faculty_data.values():\n",
        "    fName.extend(sheet_data['Preferred First Name'])\n",
        "    lName.extend(sheet_data['Preferred Last Name'])\n",
        "    fullName.extend(f\"{last} {first}\" for first, last in zip(sheet_data['Preferred First Name'], sheet_data['Preferred Last Name']))\n",
        "\n",
        "# if your excel sheet uses different column names, you will need to change the values inside the [] to match\n",
        "# For each sheet in faculty_data, we extract the 'Preferred First Name' and 'Preferred Last Name' columns.\n",
        "# We then create the fullName list by combining first and last names into one string.\n",
        "# The fName, lName, and fullName lists are extended with the respective data from each sheet.\n",
        "\n",
        "\n",
        "# Put all the full names together into the single list and remove duplicates as well as missing info (in our case the # symbol)\n",
        "empNameAgg.extend(fullName)\n",
        "empNameAgg = sorted(set(empNameAgg))\n",
        "\n",
        "for name in empNameAgg:\n",
        "  if '#' in name:\n",
        "    empNameAgg.remove(name)\n",
        "\n",
        "# list of unique last names\n",
        "uniqueLN = list(set(lName))"
      ],
      "metadata": {
        "id": "ligQt_nLc1Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optional route for missing data (this grabs the people who are missing affiliations)\n",
        "#doing it this way will take a long time (for our small set of data it was ~45mins)\n",
        "\n",
        "emplast_names = [name.split(\" \", 1)[0] for name in empNameAgg]\n",
        "empfirst_names = [name.split(\" \", 1)[1] for name in empNameAgg]\n"
      ],
      "metadata": {
        "id": "8fyipvSRAo1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2MNdrsLgVLL"
      },
      "source": [
        "## Step 3: Retrieve Researcher IDs from the Dimensions Database\n",
        "In this step, we will query the Dimensions database to retrieve information about researchers based on their last names and the research organization they belong to (in this case, the University of Cincinnati)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uriCNIHxrCZS"
      },
      "source": [
        "### Step 3.1: Define the Grid ID for the University of Cincinnati\n",
        "To query researchers at the University of Cincinnati, we first define the Grid ID for the university.\n",
        "- The `GRIDID` is the unique identifier for the University of Cincinnati in the Dimensions database. You will use this ID in the query to filter results by the university."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he8UfdkCH8d8"
      },
      "outputs": [],
      "source": [
        "# method 1\n",
        "\n",
        "# this method will search only for researchers affiliated with the given gridid (a unique value for research institutions)\n",
        "# as well as the given list of last names to narrow the search down (otherwise we get every person associated with UC which is ~33k)\n",
        "\n",
        "# The UC Grid ID\n",
        "GRIDID = 'grid.24827.3b'\n",
        "\n",
        "# Next, we define the query that will search for researchers based on their last names and the university's Grid ID.\n",
        "q = \"\"\"search researchers\n",
        "        where research_orgs = \"{}\"\n",
        "        and last_name in {}\n",
        "        return researchers\"\"\"\n",
        "\n",
        "# We execute the query and retrieve the results from the Dimensions API. The ```query_iterative``` method is\n",
        "# used to fetch the data iteratively. This is because otherwise we can only grab up to 1000 results at a time.\n",
        "\n",
        "researchers_json = dsl.query_iterative(q.format(GRIDID, json.dumps(uniqueLN)))\n",
        "\n",
        "# `q.format(GRIDID, json.dumps(uniqueLN))`: This formats the query by inserting the GRIDID and the list of last names (uniqueLN).\n",
        "# The query results are stored in the `researchers_json` variable.\n",
        "\n",
        "#convert the information we retrieved into a DataFrame object\n",
        "researchers = researchers_json.as_dataframe()\n",
        "\n",
        "# This creates a DataFrame called `researchers` that contains the details of the researchers,\n",
        "# including their names, IDs, and other basic information.\n",
        "\n",
        "# add a new column (the full name) to make a later step easier\n",
        "researchers.insert(researchers.columns.get_loc('last_name') + 1,\n",
        "                   'full_name',\n",
        "                   researchers['last_name'] + ' ' + researchers['first_name'])\n",
        "\n",
        "# The `full_name` column is inserted after the `last_name` column.\n",
        "# The `full_name` is constructed by concatenating the `last_name` and `first_name` for\n",
        "# each researcher, creating a single string that represents their full name."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# method 2\n",
        "\n",
        "# only recomended if you are searching specifically for recent data\n",
        "# this method loops through the lists of first and last names and searches the database for each one\n",
        "# it compiles this into a single list before converting it into the rquired pandas dataframe object and removing duplicates\n",
        "\n",
        "# took 43 minutes and 55 seconds to run\n",
        "#       tested with ~400 names (this includes preferred and employee name for same people)\n",
        "\n",
        "# Ensure the two lists are of equal length\n",
        "if len(empfirst_names) != len(emplast_names):\n",
        "    raise ValueError(\"First and last name lists must have the same length!\")\n",
        "\n",
        "# Initialize an empty list to store results\n",
        "all_researchers = []\n",
        "\n",
        "GRIDID = 'grid.24827.3b'\n",
        "\n",
        "# Query loop\n",
        "for first_name, last_name in zip(empfirst_names, emplast_names):\n",
        "    # Format the query\n",
        "    q = \"\"\"search researchers\n",
        "            where first_name = \"{}\"\n",
        "            and last_name = \"{}\"\n",
        "            and research_orgs = \"{}\"\n",
        "            return researchers\n",
        "            limit 1000\"\"\"\n",
        "\n",
        "    # Execute the query iteratively\n",
        "    researchers_json = dsl.query(q.format(first_name, last_name, GRIDID))\n",
        "\n",
        "    # Append results to the list\n",
        "    all_researchers+=researchers_json.researchers\n",
        "    q = \"\"\"search researchers\n",
        "            where first_name = \"{}\"\n",
        "            and last_name = \"{}\"\n",
        "            and research_orgs is empty\n",
        "            return researchers\n",
        "            limit 1000\"\"\"\n",
        "    researchers_json = dsl.query(q.format(first_name, last_name))\n",
        "\n",
        "    # Append results to the list\n",
        "    all_researchers+=researchers_json.researchers\n",
        "\n",
        "pubs_v2 = dimcli.DslDataset.from_publications_list(all_researchers)\n",
        "researchers = pubs_v2.as_dataframe()\n",
        "researchers.drop_duplicates(\"id\", inplace=True)\n",
        "\n",
        "researchers.insert(researchers.columns.get_loc('last_name') + 1,\n",
        "                   'full_name',\n",
        "                   researchers['last_name'] + ' ' + researchers['first_name'])"
      ],
      "metadata": {
        "id": "lRaDDQ6jE27q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to display the entire dataframe run this cell\n",
        "\n",
        "researchers"
      ],
      "metadata": {
        "id": "ipoiKTI_lsqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3.2: Apply the Filter to Match Faculty Members\n",
        "After creating the `full_name` column, we proceed to filter the `researchers` DataFrame by comparing each researcherâ€™s full name to the faculty members' names. This step ensures that only the relevant faculty members are retained, accounting for potential variations in how names may be stored."
      ],
      "metadata": {
        "id": "0YkR2V3nX6ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mask to filter the DataFrame\n",
        "mask = researchers.apply(\n",
        "    lambda row: (\n",
        "        # Check that 'first_name' is a valid string before applying split\n",
        "        any(\n",
        "            re.search(rf\"^{row['full_name']}\\b.*$\", name)\n",
        "            for name in set(empNameAgg + fullName)\n",
        "        )\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "# The `apply()` function iterates over each row in the ``researchers` DataFrame and applies the filter logic.\n",
        "# `re.search()` checks if the combination of the researcherâ€™s last and first name matches any name in `empNameAgg` or `fullName`.\n",
        "# Using `set(empNameAgg + fullName)` ensures that both lists are merged and duplicates are removed.\n",
        "\n",
        "\n",
        "\n",
        "# apply the filter to the DataFrame and drop any duplicate entries based on the researcher ID, last name, and first name.\n",
        "filtered_df = researchers[mask].drop_duplicates(subset=['id', 'last_name', 'first_name'], ignore_index=True)\n",
        "\n",
        "# `researchers[mask]` filters the rows that match the condition specified in the mask.\n",
        "# `drop_duplicates()` removes any rows with duplicate researcher IDs, ensuring that each researcher is listed only once.\n",
        "# `ignore_index=True` resets the index after dropping duplicates."
      ],
      "metadata": {
        "id": "PXUql2EGMs4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display the data thats been filtered\n",
        "\n",
        "filtered_df"
      ],
      "metadata": {
        "id": "LR9DFxuzRvy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Retrieving and sorting information from our filtered dataframe"
      ],
      "metadata": {
        "id": "B0sn1kOGYba4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can group the ids by the first and last name columns to combine most cases of multiple ids per a person\n",
        "\n",
        "The following is assuming we used method 1 of grabbing researchers"
      ],
      "metadata": {
        "id": "xCZ21L1to486"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "researchers_combined = filtered_df.groupby(['last_name', 'first_name', 'full_name'], as_index=False).agg({\n",
        "                                            'id': list  # Combine ids into a list\n",
        "                                          })\n",
        "# Display the result\n",
        "researchers_combined"
      ],
      "metadata": {
        "id": "wTI_0T1DwY3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By indexing or slicing with `.loc` and `.iloc`, we can combine rows of data manually that were missed or grab only the info associated with a single person (including the alternate names)."
      ],
      "metadata": {
        "id": "XciNS357V4Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example for combining 2 rows that are the same person under different first names\n",
        "# Get the single row from the DataFrame\n",
        "single_researcher = researchers_combined.loc[[4], ['first_name', 'last_name', 'id']]\n",
        "\n",
        "# Append the new ID (from the 5th row) to the list in 'id'\n",
        "single_researcher['id'].iloc[0].extend(researchers_combined['id'].iloc[5])"
      ],
      "metadata": {
        "id": "b9N2tV3NaPAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the updated single_researcher\n",
        "single_researcher"
      ],
      "metadata": {
        "id": "UNzxj7c9wbRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some people have multiple researcher IDs so this needs manually verified that they are the same or different people\n"
      ],
      "metadata": {
        "id": "Wq5coGIGPvKy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r12p1jbFrv3p"
      },
      "source": [
        "## This retrieves information on the publications by the authors in the filtered_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All researcher ids"
      ],
      "metadata": {
        "id": "DEe93u73ZCBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use this for a list of all the ids for all of our researchers\n",
        "researchers_ids = filtered_df['id']\n",
        "researchers_ids\n",
        "\n",
        "r_id = list(filtered_df['id'])\n",
        "r_id"
      ],
      "metadata": {
        "id": "Z7YRcMzArBC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively create a list for the specific ids you wish to search"
      ],
      "metadata": {
        "id": "uNU2HiRFZDKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# use this if you are searching a single researcher using the above info\n",
        "r_id = list(itertools.chain.from_iterable(single_researcher['id']))\n",
        "r_id"
      ],
      "metadata": {
        "id": "s-d3FmzsQajO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AffUlsP6H-fx"
      },
      "outputs": [],
      "source": [
        "# Adjust the number of researchers IDs per query: so to ensure we never hit the 1000 records limit per query\n",
        "# this is doable by slicing the r_id list (mainly needed if you are grabbing all the data)\n",
        "#     you can also change the query to grab the data iteratively:\n",
        "#         data = dsl.query_iterative(q.format(json.dumps(r_id),json.dumps(years)))\n",
        "\n",
        "\n",
        "# adjust the years here if you want to look at years different from the earlier list (2018-2024)\n",
        "#years = []\n",
        "\n",
        "q = \"\"\"search publications\n",
        "       where researchers.id in {}\n",
        "       and year in {}\n",
        "       return publications\n",
        "       limit 1000\"\"\"\n",
        "\n",
        "#change the value of r_id or use a different variable to adjust which person is being looked at\n",
        "data = dsl.query(q.format(json.dumps(r_id),json.dumps(years)))\n",
        "data_df = data.as_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets add a new column to the publication's dataframe.\n",
        "\n",
        "The following cell will extract all the associated researcher ids with the given publications and add them to the data\n",
        "\n",
        "The way it does this is to duplicate each row for each discovered researcher_id in the author column so that there is 1 or more rows for each publication dependent on the number of researchers."
      ],
      "metadata": {
        "id": "OKdSqyhiJMuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract nested researcher_ids\n",
        "nested_researcher_ids = [\n",
        "    [author[\"researcher_id\"] for author in author_list]\n",
        "    for author_list in data_df[\"authors\"]\n",
        "]\n",
        "\n",
        "# Flatten the nested researcher_ids into a single list\n",
        "flattened_researcher_ids = [researcher_id for ids in nested_researcher_ids for researcher_id in ids]\n",
        "\n",
        "# Create a new DataFrame with expanded rows\n",
        "expanded_df = data_df.loc[data_df.index.repeat([len(ids) for ids in nested_researcher_ids])].reset_index(drop=True)\n",
        "\n",
        "# Add the `r_id` column at the correct position after `id`\n",
        "expanded_df.insert(\n",
        "    data_df.columns.get_loc(\"id\") + 1,  # Position after 'id'\n",
        "    \"r_id\",\n",
        "    flattened_researcher_ids\n",
        ")\n",
        "\n",
        "# Drop the original authors column (optional)\n",
        "#expanded_df = expanded_df.drop(columns=[\"authors\"])\n",
        "\n",
        "expanded_df"
      ],
      "metadata": {
        "id": "gDRUAWUvi08v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expanded_df[['id', 'r_id', 'title', 'type', 'year']]"
      ],
      "metadata": {
        "id": "PVHqw_bxGEaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this lets us get only the researcher ids sorted by publication id\n",
        "# you can alternatly inverse the \"id\" and 'r_id' to get each publication grouped by the researcher_id\n",
        "expanded_df.groupby([\"r_id\"]).agg({'id': list  # Combine ids into a list\n",
        "                                          })"
      ],
      "metadata": {
        "id": "n7GaU5Rfj15N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F6h46LZOGzwK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}